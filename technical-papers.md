# Technical Papers worth reading

Technical discussions where a group of folks sit down and talk about interesting technical papers is probably one of the best ways to build a robust learning community. One of the first steps to do that is identifying some papers worth chatting about that can create lively discussion. Most of the suggestions below have come via the excellent list from [Will Larson](https://lethain.com/about/) the author of [An Elegant Puzzle](https://www.amazon.com/Elegant-Puzzle-Systems-Engineering-Management/dp/1732265186)


## The must reads
* [Big Ball of Mud](http://www.laputan.org/mud/) or [PDF](https://s3.amazonaws.com/systemsandpapers/papers/bigballofmud.pdf) Probably the best paper on system architecture in the real world ever.

* [Fielding's REST Paper](https://www.ics.uci.edu/~fielding/pubs/dissertation/rest_arch_style.htm) The full orignal can be found [here](https://www.ics.uci.edu/~fielding/pubs/dissertation/fielding_dissertation.pdf). Its also worth reading [Roy Fielding's Misappropriated REST Dissertation](https://twobithistory.org/2020/06/28/rest.html) for context on how REST is often abused in the real world.

* [On Designing and Deploying Internet-Scale Services](https://s3.amazonaws.com/systemsandpapers/papers/hamilton.pdf) The system-to-administrator ratio is commonly used as a rough metric to understand administrative costs in high-scale services. With smaller, less automated services this ratio can be as low as 2:1, whereas on industry leading, highly automated services, there are ratios as high as 2,500:1.  Services that are operations friendly require little human intervention, and both detect and recover from all but the most obscure failures without administrative intervention. This paper summarizes the best practices accumulated over many years in scaling some of the largest services at MSN and Windows Live. This is a true checklist of how to design and evaluate large scale systems (almost like [The Twelve Factor App](https://12factor.net/) wants to be for a checklist for operable applications).

* [CAP Twelve Years Later: How the Rules Have Changed](https://www.infoq.com/articles/cap-twelve-years-later-how-the-rules-have-changed) Eric Brewer posited the [CAP theorem](https://en.wikipedia.org/wiki/CAP_theorem) in the early 2000s, and twelve years later he wrote this excellent overview and review of CAP (which argues distributed systems have to pick between either availability or consistency during partitions), in part because: In the decade since its introduction, designers and researchers have used (and sometimes abused) the CAP theorem as a reason to explore a wide variety of novel distributed systems. The NoSQL movement also has applied it as an argument against traditional databases. CAP is interesting because there is not a “seminal CAP paper”, but this article serves well in such a paper’s stead. These ideas are expanded on in the _Harvest and Yield paper_ below.

* [Harvest, Yield, and Scalable Tolerant Systems](https://s3.amazonaws.com/systemsandpapers/papers/FOX_Brewer_99-Harvest_Yield_and_Scalable_Tolerant_Systems.pdf) This paper builds on the concepts from [CAP Twelve Years Later](/papers/883d1956-bb05-464c-b02d-5a64f269c810/), introducing the concepts of harvest and yield to add more nuance to the AP vs CP discussion. The harvest and yield concepts are particularly interesting because they are both self-evidence and very rarely explicitly used, instead distributed systems continue to fail in mostly undefined ways. 

* [Searching for Build Debt: Experiences Managing Technical Debt at Google](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37755.pdf) This paper is an interesting cover of how to perform large-scale migrations in living codebases. Google software engineers are constantly paying interest on various forms of technical debt. Google engineers also make efforts to pay down that debt, whether through special Fixit days, or via dedicated teams, variously known as janitors, cultivators, or demolition experts. The paper describe several related efforts to measure and pay down technical debt found in Google’s BUILD files and associated dead code. It also addresses debt found in dependency specifications, unbuildable targets, and unnecessary command line flags. These efforts often expose other forms of technical debt that must first be managed. Using broken builds as the running example, they break down their strategy to three pillars: automation, make it easy to do the right thing, and make it hard to do the wrong thing.

* [Dynamo: Amazon’s Highly Available Key-value Store](https://s3.amazonaws.com/systemsandpapers/papers/amazon-dynamo-sosp2007.pdf) This paper is a phenomenal introduction to eventual consistency, coordinating state across distributed storage, reconciling data as it diverges across replicas and much more.

* [Design patterns for container-based distributed systems](https://s3.amazonaws.com/systemsandpapers/papers/design-container-based-systems.pdf) The move to containers-based deployment and orchestration has introduced a whole new set of vocabulary like sidecars and adapters, and this paper provides a survey of the patterns which have evolved over the past decade as microservices and containers have become increasingly prominent infrastructure components The “sidecar” term in particular likely originated in [this blog post from Netflix](http://techblog.netflix.com/2014/11/prana-sidecar-for-your-netflix-paas.html), which is a worthy read in its own right.



## The best of the rest

* [Hints for Computer System Design](https://s3.amazonaws.com/systemsandpapers/papers/acrobat-17.pdf) [Butler Lampson](https://en.wikipedia.org/wiki/Butler_Lampson) is an ACM Turning Award winner (among other awards), and worked at the Xerox PARC. This paper concisely summarizes many of his ideas around system design, and is a great read.

* [The Google File System](https://s3.amazonaws.com/systemsandpapers/papers/gfs.pdf) The inspiration for the ground breaking Hadoop File system.

* [MapReduce: Simplified Data Processing on Large Clusters](https://s3.amazonaws.com/systemsandpapers/papers/mapreduce.pdf) The MapReduce paper is an excellent example of an idea which has been so successful that it now seems self-evident. The idea of applying the concepts of functional programming at scale became a clarion call, provoking a shift from data warehousing to a new paradigm for data analysis, this paper was itself a major inspiration for Hadoop.

* [Dapper, a Large-Scale Distributed Systems Tracing Infrastructure](https://s3.amazonaws.com/systemsandpapers/papers/dapper.pdf) The Dapper paper introduces a performant approach to tracing requests across many services, which has become increasingly relevant as more companies refactor core monolithic applications into dozens or hundreds of micro-services. Introduces the design of Dapper, Google’s production distributed systems tracing infrastructure, and describes how the design goals of low overhead, application-level transparency, and ubiquitous deployment on a very large scale system were met. Dapper shares conceptual similarities with other tracing systems, particularly Magpie and X-Trace. However certain design choices were made that have been key to its success, such as the use of sampling and restricting the instrumentation to a rather small number of common libraries. The ideas from Dapper have since made their way into open source, especially in [Zipkin](http://zipkin.io/) and [OpenTracing](http://opentracing.io/).

* [Kafka: a Distributed Messaging System for Log Processing](https://s3.amazonaws.com/systemsandpapers/papers/Kafka.pdf) [Apache Kafka](https://kafka.apache.org/) has become a core piece of infrastructure for many internet companies. It’s versatility lends it to many roles, serving as the ingress point to “data land” for some, a durable queue for others, and that’s just scratching the surface. Not only a useful addition to your toolkit, Kafka is also a beautifully designed system. In particular, Kafka’s partitions do a phenomenal job of forcing application designers to make explicit tradeoffs about trading off performance for predictable message ordering.

* [Wormhole: Reliable Pub-Sub to Support Geo-replicated Internet Services](https://s3.amazonaws.com/systemsandpapers/papers/wormhole.pdf) In many ways similar to [Kafka](/papers/910e84cb-337c-4569-805f-6df67b23c443/), Facebook’s Wormhole is another highly scalable approach to messaging. In particular, note the approach to supporting lagging consumers without sacrificing overall system throughput.

* [Borg, Omega, and Kubernetes](http://queue.acm.org/detail.cfm?id=2898444) While the individual papers for each of Google’s orchestration systems (Borg, Omega and Kubernetes) are worth reading in their own right, this article is an excellent overview of the three. This article describes the lessons learned from developing and operating them.

* [Large-scale cluster management at Google with Borg](https://s3.amazonaws.com/systemsandpapers/papers/borg.pdf) Borg has been orchestrating much of Google’s infrastructure for quite some time (significantly predating Omega, although fascinatingly the Omega paper predates the Borg paper by two years). Google’s Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines. This paper takes a look at Borg’s centralized scheduling model, which was both effective and efficient, although it became increasingly challenging to modify and scale over time, inspiring both Omega and Kubernetes within Google (the former to optimistically replace it, and the later seemingly to commercialize their learnings, or at least prevent Mesos from capturing too much mindshare).

* [Omega: flexible, scalable schedulers for large compute clusters](https://s3.amazonaws.com/systemsandpapers/papers/omega.pdf) Omega is, among many other things, an excellent example of the [second-system effect](http://catb.org/jargon/html/S/second-system-effect.html), where an attempt to replace a complex existing system with something far more elegant ends up being more challenging than anticipated. In particular, Omega is a reaction against the realities of extending the aging Borg system. Perhaps also an example of [worse is better](https://www.jwz.org/doc/worse-is-better.html) once again taking the day.

* [Mesos: A Platform for Fine-Grained Resource Sharing in the Data Center](https://s3.amazonaws.com/systemsandpapers/papers/mesos.pdf) This paper describes the design of [Apache Mesos](http://mesos.apache.org/), in particular its distinctive two-level scheduler. Used heavily by Twitter and Apple, Mesos was for some time the only open-source general scheduler with significant adoption, and is now in a fascinating competition for mindshare with Kubernetes.

* [Raft: In Search of an Understandable Consensus Algorithm](https://s3.amazonaws.com/systemsandpapers/papers/raft.pdf) and [Paxos Made Simple](https://s3.amazonaws.com/systemsandpapers/papers/paxos-made-simple.pdf). Rather complex papers on concensus algorithms. Raft is simpler and is used by [etcd](https://github.com/coreos/etcd) and [influxdb](https://www.influxdata.com/) among many others. Paxos is a deeply innovative concept, and is the algorithm behind Google’s Chubby and [Apache Zookeeper](https://zookeeper.apache.org/), among many others.

* [SWIM: Scalable Weakly-consistent Infection-style Process Group Membership Protocol](https://s3.amazonaws.com/systemsandpapers/papers/swim.pdf) The majority of consensus algorithms focus on being consistent during partition, but SWIM goes the other direction and focuses on availability. Several distributed peer-to-peer applications require weakly-consistent knowledge of process group membership information at all participating processes. SWIM is a generic software module that offers this service for large scale process groups. The SWIM effort is motivated by the unscalability of traditional heart-beating protocols, which either impose network loads that grow quadratically with group size, or compromise response times or false positive frequency w.r.t. detecting process crashes. This paper reports on the design, implementation and performance of the SWIM sub-system on a large cluster of commodity PCs. SWIM is used in Hashicorp’s software, as well as [Uber’s Ringpop](http://uber.github.io/ringpop/).

* [Out of the Tar Pit](https://s3.amazonaws.com/systemsandpapers/papers/outofthetarpit.pdf) Out of the Tar Pit bemoans unnecessary complexity in software, and proposes that that functional programming and better data modeling can help us reduce accidental complexity (arguing that most unnecessary complexity comes from state). Certainly a good read. Reading it a decade later it’s fascinating to see that neither of those approaches have taken off. Instead the closest “universal” approach to reducing complexity seems to be the move to numerous mostly stateless services. Which is perhaps more a reduction of _local complexity_, at the expense of larger systemic complexity, whose maintenance is then delegated to more specialized systems engineers.

* [Bigtable: A Distributed Storage System for Structured Data](https://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf) One of Google’s preeminent papers and technologies is Bigtable, which was an early (early in the internet era, anyway) NoSQL datastore, operating at extremely high scale and built on top of [Chubby](https://s3.amazonaws.com/systemsandpapers/papers/chubby-osdi06.pdf). From the SSTable design to the bloom filters, Cassandra inherits significantly from the Bigtable paper, and is probably rightfully considered a merging of the Dynamo and Bigtable papers.

* [Spanner: Google’s Globally-Distributed Database](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44915.pdf) Where many early NoSQL storage systems traded eventual consistency for increased resiliency, building on top of eventually consistent systems can be harrowing. Spanner represents an approach from Google to offering both strong consistency and distributed reliability, relying in part on a novel approach to managing time.

* [Security Keys: Practical Cryptographic Second Factors for the Modern Web](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45409.pdf) [Security keys like the YubiKey](https://en.wikipedia.org/wiki/YubiKey) have emerged as the most secure second authentication factor, and this paper out of Google explains the motivations that lead to their creation, and the design that makes them work. They’re also remarkably cheap. 

* [BeyondCorp: Design to Deployment at Google](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/44860.pdf) Building on the [original BeyondCorp paper](https://www.usenix.org/system/files/login/articles/login_dec14_02_ward.pdf) in 2014, this paper is slightly more detailed and benefits from two more years of migration-fueled wisdom. That said, the big ideas have remained fairly consistent and there is not much new relative to the BeyondCorp paper itself.

* [Availability in Globally Distributed Storage Systems](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36737.pdf) This paper explores how to think about availability in replicated distributed systems, and is a useful starting point for those of us who are trying to determine the correct way to measure uptime for their storage layer or any other sufficiently complex system. A simple definition of availability was in this case: A storage node becomes unavailable when it fails to respond positively to periodic health checking pings sent by our monitoring system. The node remains unavailable until it regains responsiveness or the storage system reconstructs the data from other surviving nodes. Often discussions of availability become arbitrarily complex so this is a good exmaple of simplicity.

* [Still All on One Server: Perforce at Scale](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/39983.pdf) This paper is particularly good at considering the difficulties that companies run into scaling Git monorepos.

* [Source Code Rejuvenation is not Refactoring](https://s3.amazonaws.com/systemsandpapers/papers/sofsem10.pdf) This paper introduces the concept of “Code Rejuvenation”, a unidirectional process of moving towards cleaner abstractions as new language features and libraries become available, which is particularly applicable to sprawling, older code bases. There are some strong echoes of this work in [Google’s ClangMR paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41342.pdf).


---

## Additional amazing papers:

*   [Amazon Aurora: Design Considerations for High Throughput Cloud-Native Relational Databases](https://www.allthingsdistributed.com/files/p1041-verbitski.pdf)
*   [Canopy: An End-to-End Performance Tracing And Analysis System](https://research.fb.com/wp-content/uploads/2017/10/sosp17-final14.pdf)
*   [Unreliable Failure Detectors for Reliable Distributed Systems](https://www.cs.utexas.edu/~lorenzo/corsi/cs380d/papers/p225-chandra.pdf)
*   [CRDTs: Consistency without concurrency control](https://hal.inria.fr/inria-00397981/document)
*   [Logic and Lattices for Distributed Programming](http://db.cs.berkeley.edu/papers/UCB-lattice-tr.pdf)
*   [Zab: High-performance broadcast for primary-backup systems](https://pdfs.semanticscholar.org/b02c/6b00bd5dbdbd951fddb00b906c82fa80f0b3.pdf)
*   [Paxos Made Live - An Engineering Perspective](https://static.googleusercontent.com/media/research.google.com/en//archive/paxos_made_live.pdf)
*   [Towards a Solution to the Red Wedding Problem](http://christophermeiklejohn.com/publications/hotedge-2018-preprint.pdf)
*   [Profiling a warehouse-scale computer](https://research.google.com/pubs/pub44271.html)
*   [Google-Wide Profiling: A Continuous Profiling Infrastructure for Data Centers](https://research.google.com/pubs/pub36575.html)
*   [A Few Billion Lines of Code Later: Using Static Analysis to Find Bugs in the Real World](http://delivery.acm.org/10.1145/1650000/1646374/p66-bessey.pdf?ip=67.170.235.99&id=1646374&acc=OPEN&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E6D218144511F3437&__acm__=1523208080_0e8b4d3f89204b7f865ffb5e4c8675df)
*   [Rules of Machine Learning: Best Practices for ML Engineering](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf)
*   [A comprehensive study of Convergent and Commutative Replicated Data Types](https://hal.inria.fr/file/index/docid/555588/filename/techreport.pdf)
*   [Online, Asynchronous Schema Change in F1](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/41376.pdf)
*   [Time, Clocks and the Ordering of Events in a Distributed System](https://amturing.acm.org/p558-lamport.pdf)
*   [C-Store: A Column-oriented DBMS](http://www.cs.yale.edu/homes/dna/vldb.pdf)
*   [Life beyond Distributed Transactions: an Apostate’s Opinion](http://adrianmarriott.net/logosroot/papers/LifeBeyondTxns.pdf)
*   [Immutability Changes Everything](http://cidrdb.org/cidr2015/Papers/CIDR15_Paper16.pdf)
*   [Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks](https://www.cse.buffalo.edu/~stevko/courses/cse704/fall10/papers/eurosys07.pdf)
*   [Basic Local Alignment Search Tool](http://bio.cs.washington.edu/blast.pdf)